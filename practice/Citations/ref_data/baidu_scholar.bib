
@article{jurgen_schmidhuber_deep_2015,
	title = {Deep learning in neural networks},
	url = {http://dl.acm.org/citation.cfm?id=2947734"},
	doi = {10.1016/J.NEUNET.2014.09.003},
	urldate = {2023-02-02},
	journal = {Neural Netw},
	author = {{Jürgen Schmidhuber}},
	year = {2015},
	keywords = {Deep learning, Evolutionary computation, Reinforcement learning, Supervised learning, Unsupervised learning},
	file = {已提交版本:C\:\\Users\\lenovo\\Zotero\\storage\\MJ4C9NQF\\Jürgen Schmidhuber - 2015 - Deep learning in neural networks.pdf:application/pdf;Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\RCX4LQYC\\s.html:text/html},
}

@article{erhan_why_2010,
	title = {Why {Does} {Unsupervised} {Pre}-training {Help} {Deep} {Learning}?},
	volume = {11},
	url = {http://dl.acm.org/doi/10.5555/1756006.1756025},
	doi = {10.1145/1756006.1756025},
	abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
	number = {3},
	urldate = {2023-02-02},
	journal = {Journal of Machine Learning Research},
	author = {Erhan, D. and Bengio, Y. and Courville, A. and Manzagol, P. A. and Vincent, P. and Bengio, S.},
	year = {2010},
	keywords = {Aaron Courville, CiteSeerX, Dumitru Erhan, Pascal Vincent, Yoshua Bengio},
	pages = {625--660},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\47EM2TZM\\s.html:text/html},
}

@inproceedings{ngiam_multimodal_2009,
	title = {Multimodal {Deep} {Learning}},
	url = {http://www.mendeley.com/research/multimodal-deep-learning-1/},
	abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning. 1.},
	urldate = {2023-02-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Ngiam, J. and Khosla, A. and Kim, M. and Nam, J. and Ng, A. Y.},
	year = {2009},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\RSP3BPFD\\s.html:text/html},
}

@article{geert_survey_2017,
	title = {A survey on deep learning in medical image analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841517301135},
	doi = {10.1016/j.media.2017.07.005},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research},
	urldate = {2023-02-02},
	journal = {Medical Image Analysis},
	author = {{Geert} and {Litjens} and {Thijs} and {Kooi} and {Babak} and {Ehteshami} and {Bejnordi} and {Arnaud} and {Arindra} and {Adiyoso}},
	year = {2017},
	keywords = {Convolutional, Deep, imaging, learning, Medical, networks, neural, Survey},
	file = {全文:C\:\\Users\\lenovo\\Zotero\\storage\\KLNB34KY\\Geert 等 - 2017 - A survey on deep learning in medical image analysi.pdf:application/pdf;Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\IL9WE537\\s.html:text/html},
}

@inproceedings{deng_foundations_2014,
	title = {Foundations and {Trends} in {Signal} {Processing}: {DEEP} {LEARNING} - {Methods} and {Applications}},
	shorttitle = {Foundations and {Trends} in {Signal} {Processing}},
	url = {http://msr-waypoint.com/pubs/219984/DeepLearningBook_RefsByLastFirstNames.pdf},
	abstract = {Automatic speech recognition : a deep learning approach Dong Yu, Li Deng (Signals and communication technology) Springer, c2015 : [hardback]},
	urldate = {2023-02-02},
	booktitle = {Springer {Publishing} {Company}, {Incorporated}},
	author = {Deng, L.},
	year = {2014},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\AAT6F7QR\\s.html:text/html},
}

@article{babak_predicting_2015,
	title = {Predicting the sequence specificities of {DNA}- and {RNA}-binding proteins by deep learning.},
	doi = {10.1038/nbt.3300},
	abstract = {Knowing the sequence specificities of DNA- and RNA-binding proteins is essential for developing models of the regulatory processes in biological systems and for identifying causal disease variants. Here we show that sequence specificities can be ascertained from experimental data with 'deep learning' techniques, which offer a scalable, flexible and unified computational approach for pattern discovery. Using a diverse array of experimental data and evaluation metrics, we find that deep learning outperforms other state-of-the-art methods, even when training on in vitro data and testing on in vivo data. We call this approach DeepBind and have built a stand-alone software tool that is fully automatic and handles millions of sequences per experiment. Specificities determined by DeepBind are readily visualized as a weighted ensemble of position weight matrices or as a 'mutation map' that indicates how variations affect binding within a specific sequence.},
	journal = {Nature biotechnology},
	author = {{Babak} and {Alipanahi} and {Andrew} and {Delong} and Matthew, T. and {Weirauch} and Brendan, J. and {Frey}},
	year = {2015},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\A3Y9B4YF\\s.html:text/html},
}

@article{chetlur_cudnn_2014,
	title = {{cuDNN}: {Efficient} {Primitives} for {Deep} {Learning}},
	shorttitle = {{cuDNN}},
	url = {http://arxiv.org/abs/1410.0759},
	doi = {http://hgpu.org/?p=12909},
	abstract = {We present a library that provides optimized implementations for deep learning primitives. Deep learning workloads are computationally intensive, and optimizing the kernels of deep learning workloads is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized for new processors, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, and similarly to the BLAS library, could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
	urldate = {2023-02-02},
	journal = {Computer ence},
	author = {Chetlur, S. and Woolley, C. and Vandermersch, P. and Cohen, J. and Shelhamer, E.},
	year = {2014},
	keywords = {Computer Science - Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\AYUD8EVJ\\s.html:text/html},
}

@article{madry_towards_2017,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	doi = {10.48550/arXiv.1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL  and this https URL .},
	urldate = {2023-02-02},
	author = {Madry, A. and Makelov, A. and Schmidt, L. and Tsipras, D. and Vladu, A.},
	year = {2017},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\VVGN94JU\\s.html:text/html},
}

@article{bengio_deep_2011,
	title = {Deep {Learning} of {Representations} for {Unsupervised} and {Transfer} {Learning}},
	url = {http://www.researchgate.net/publication/228618750_Deep_learning_of_representations_for_unsupervised_and_transfer_learning},
	abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution \$P(x)\$ is structurally related to some task of interest, say predicting \$P(y{\textbar}x)\$. This paper focusses on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
	urldate = {2023-02-02},
	journal = {workshop on unsupervised \& transfer learning},
	author = {Bengio, Y. and Guyon, G. and Dror, V. and Lemaire, G. and {Silver}},
	year = {2011},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\CFMDZJEY\\s.html:text/html},
}

@inproceedings{ouyang_joint_2014,
	title = {Joint {Deep} {Learning} for {Pedestrian} {Detection}},
	url = {http://ieeexplore.ieee.org/document/6751366/},
	doi = {10.1109/ICCV.2013.257},
	abstract = {Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture. By establishing automatic, mutual interaction among components, the deep model achieves a 9\% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.},
	urldate = {2023-02-02},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Ouyang, W. and Wang, X.},
	year = {2014},
	file = {Snapshot:C\:\\Users\\lenovo\\Zotero\\storage\\B7S6YGIS\\s.html:text/html},
}
